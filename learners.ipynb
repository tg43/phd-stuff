{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81529482",
   "metadata": {},
   "source": [
    "# Machine Learning, Composability, and the Category Learn\n",
    "A neural network, in practice, is often interpreted and constructed as a composition of multiple predefined components that serve as \"atoms\" from which to build increasingly complexl models. Naturally, machine learning libraries often feature these components as built-ins, but this implicitly uses the composability of these \"trainable\" components.\n",
    "(Fong et al., 2019) formalise this notion by describing \"trainable\" functions as morphisms in a suitably constructed category, the category **Learn**. Here, we introduce their concept of a *learner* by constructing a small neural network within Julia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f8ea9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random\n",
    "\n",
    "abstract type Learner{M, N} end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db632f3",
   "metadata": {},
   "source": [
    "## Learners\n",
    "The problem of supervised learning is to find suitable approximation for a target function $f^* : A \\to B$ within a collection of functions $f : P \\times A \\to B$ where $P$ is a parameter space. Precisely, by looking at examples $(a, b) \\in A \\times B$ with $f^*(a) \\approx b$, we want to discover a choice of parameters $p^* \\in P$ such that $f^* \\approx f(p^*, \\cdot)$ according to some error metric. (Fong et al., 2019) constructs *learners* to contain the necessary information to carry out this task, including:\n",
    " - Parameter Space: $P$\n",
    " - Implementation Function: $I : P \\times A \\to B$\n",
    " - Update Function: $U : P \\times A \\times B \\to P$, and\n",
    " - Request Function: $r : P \\times A \\times B \\to A$.\n",
    "\n",
    "Intuitively, a learner can be throught of as a family of parameterised functions equipped with a method for updating its current parameters according to obsevations.\n",
    "\n",
    "### What a Learner Does (Implementation Functions)\n",
    "First, to specify a learner from an input space $A$ to an output space $B$, we need to describe the relationship it implements between these inputs and outputs. This relationship is allowed to depend on parameters taken from the learner's parameter space $P$ and is captured by its implementation function $I : P \\times A \\to B$. We view the implementation function as carrying a parameter $p \\in P$ and an input $a \\in A$ to a predicted output $I(p, a) \\in B$.\n",
    "\n",
    "Consider, for instance, the simple example of a fully-connected linear layer from $\\mathbb{R}^M$ to $\\mathbb{R}^N$ that has:\n",
    " - a parameter space $P = \\mathbb{R}^{N \\times M} \\times \\mathbb{R}^N$ containing all $N \\times M$ weight matrices and all $N \\times 1$ bias vectors, and\n",
    " - an implementation function where, for any parameters $p = (W, b) \\in P$, we have\n",
    "$$ I(p, x) = W x + b $$\n",
    "   for all $x \\in \\mathbb{R}^M$.\n",
    "\n",
    "We can implement this learner in Julia as a type `LinearLearner{M, N}` where instances of this type represent choices of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7bc7c2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "implementation (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct LinearLearner{M, N} <: Learner{M, N}\n",
    "    weights::Matrix{Float64}\n",
    "    biases::Vector{Float64}\n",
    "    \n",
    "    function LinearLearner{M, N}(weights, biases) where {M, N}\n",
    "        @assert size(weights) == (N, M)\n",
    "        @assert size(biases) == (N,)\n",
    "        return new(weights, biases)\n",
    "    end\n",
    "end\n",
    "\n",
    "function LinearLearner{M, N}() where {M, N}\n",
    "    weights = rand(N, M)\n",
    "    biases = rand(N)\n",
    "    return LinearLearner{M, N}(weights, biases)\n",
    "end\n",
    "\n",
    "function implementation(learner::LinearLearner{M, N}, input::Vector{Float64}) where {M, N}\n",
    "    @assert size(input) == (M,)\n",
    "    return learner.weights * input + learner.biases\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2fc13c",
   "metadata": {},
   "source": [
    "We will, of course, also need to incorporate some nonlinearity by introducing the sigmoid activation function, which can be described as its own separate learner. Note that, since this activation function has no trainable parameters, the learner's parameter space is an arbitrary singleton set $\\{\\star\\}$. Its implementation function is given by $I(\\star, x) = (\\sigma(x_i))_i$ for all $x \\in \\mathbb{R}^N$, where $\\sigma : \\mathbb{R} \\to \\mathbb{R}$ denotes the sigmoid function. We represent a sigmoid learner from $\\mathbb{R}^N$ to $\\mathbb{R}^N$ by the `SigmoidLearner{N}` type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c81bb60e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "implementation (generic function with 2 methods)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "σ(x::Float64) = 1 / (1 + exp(-x))\n",
    "Dσ(x::Float64) = σ(x) * (1 - σ(x))\n",
    "\n",
    "struct SigmoidLearner{N} <: Learner{N, N}\n",
    "end\n",
    "\n",
    "function implementation(learner::SigmoidLearner{N}, input::Vector{Float64}) where {N}\n",
    "    @assert size(input) == (N,)\n",
    "    return σ.(input)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f96d45f",
   "metadata": {},
   "source": [
    "### How a Learner Learns (Update Functions)\n",
    "Now, having created `LinearLearner{M, N}` and `SigmoidLearner{N}`, we need to give our learners a way to learn from observation. This is accomplished by an update function $U : P \\times A \\times B \\to P$ that takes a current parameter $p \\in P$ and an observation $(a, b) \\in A \\times B$ and gives an updated parameter $U(p, a, b) \\in P$ that should—although this is not strictly necessary—improve on the current approximation.\n",
    "\n",
    "We need to introduce a learning rate $\\epsilon > 0$ and a function $e : \\mathbb{R} \\times \\mathbb{R} \\to \\mathbb{R}$ such that $E_I(p, a, b) = \\sum_j e(I_j(p, a) ,b_j)$ computes the error between a predicted output $I(p, a)$ and an actual output $b$. Then, we compute the derivative of this error with respect to each the parameters and nudge this parameters to reduce this error. Mathematically, our update function can be generally formulated as\n",
    "$$ U(p, a, b) = p - \\epsilon \\nabla_p E_I(p, a, b)$$\n",
    "for all $p \\in P$, $a \\in A$, and $b \\in B$. Below, we specialise this formula to `LinearLearner{M, N}` using quadratic loss $e(x, y) = \\frac{1}{2} (x - y)^2$. Observe that, because `SigmoidLearner{N}` has no meaningful parameterrs, its update function does not actually change anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ded136e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "update (generic function with 2 methods)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ϵ = 0.1\n",
    "\n",
    "function update(\n",
    "        learner::LinearLearner{M, N},\n",
    "        input::Vector{Float64},\n",
    "        expected::Vector{Float64}\n",
    ") where {M, N}\n",
    "    @assert size(input) == (M,)\n",
    "    @assert size(expected) == (N,)\n",
    "\n",
    "    predicted = implementation(learner, input)\n",
    "    weights = learner.weights - ϵ * (predicted - expected) * input'\n",
    "    biases = learner.biases - ϵ * (predicted - expected)\n",
    "\n",
    "    return LinearLearner{M, N}(weights, biases)\n",
    "end\n",
    "\n",
    "function update(\n",
    "    learner::SigmoidLearner{N},\n",
    "    input::Vector{Float64},\n",
    "    expected::Vector{Float64}\n",
    ") where {N}\n",
    "    @assert size(input) == (N,)\n",
    "    @assert size(expected) == (N,)\n",
    "\n",
    "    return learner\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a92b6d6",
   "metadata": {},
   "source": [
    "The introduction of these update functions allows us to already implement a function for training a learner. This is accomplished by the `train` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29979c3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step(learner, datum) = update(learner, datum.input, datum.output)\n",
    "train(learner, data, epochs) = foldl(step, Iterators.repeat(data, epochs); init=learner)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ad0f9a",
   "metadata": {},
   "source": [
    "We can verify that the update function for our `LinearLearner{M, N}` is indeed improving the parameter choices by testing it on a simple example. Here, we generate a linear target function from $\\mathbb{R}^2$ to $\\mathbb{R}^2$ and a `LinearLearner{2, 2}` find the underlying weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e85971c2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Weights: [0.41 0.86; 0.07 0.09]\n",
      "Target Biases: [0.66, 0.12]\n",
      "\n",
      "Trained Weights: [0.41 0.86; 0.07 0.09]\n",
      "Trained Biases: [0.66, 0.12]\n"
     ]
    }
   ],
   "source": [
    "Random.seed!(0)\n",
    "\n",
    "input_size = 2\n",
    "output_size = 2\n",
    "samples = 100\n",
    "epochs = 100\n",
    "\n",
    "# Target Function\n",
    "W = round.(rand(output_size, input_size); digits=2)\n",
    "b = round.(rand(output_size); digits=2)\n",
    "f(x) = W * x + b\n",
    "\n",
    "# Training\n",
    "data = [(input=Vector(x), output=f(Vector(x))) for x in eachcol(rand(input_size, samples))]\n",
    "learner = LinearLearner{2, 2}()\n",
    "result = train(learner, data, epochs)\n",
    "\n",
    "println(\"Target Weights: $(W)\")\n",
    "println(\"Target Biases: $(b)\")\n",
    "println()\n",
    "println(\"Trained Weights: $(round.(result.weights; digits=2))\")\n",
    "println(\"Trained Biases: $(round.(result.biases; digits=2))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893ba5a6",
   "metadata": {},
   "source": [
    "### Composing Learners (Request Functions)\n",
    "So, we are ready to combine our learners to create arbitrarily complex networks, right? Below, we use the notation $(P, I, U, r)$ and $(Q, J, V, s)$ to represent a pair of learners from $A$ to $B$ and from $B$ to $C$, respectively.\n",
    "We want to find their composite\n",
    "$$ (Q, J, V, s) \\circ (P, I, U, r). $$\n",
    "This means that we need to give a parameter space, implementation function, update function, and (yet to be introduced) request function for the resulting learner. Let's try and create a `CompositeLearner{M, N}` type that represents this composition of multiple learners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d020878",
   "metadata": {},
   "outputs": [],
   "source": [
    "struct CompositeLearner{M, H, N} <: Learner{M, N}\n",
    "    right::Learner{M, H}\n",
    "    left::Learner{H, N}\n",
    "end\n",
    "\n",
    "function Base.:∘(left::Learner{M, H}, right::Learner{H, N}) where {M, H, N}\n",
    "    return CompositeLearner{M, H, N}(right, left)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b213d68e",
   "metadata": {},
   "source": [
    "The choice of parameter space is straightforward and, because we need to store the details necessary to parameterise both learners, we choose $P \\circ Q = P \\times Q$ to be the parameter space of their composite. Similarly, the implementation function is given simply by the function composition of $I$ and $J$, that is, we have\n",
    "$$ (I \\circ J)((p, q), a) = J(q, I(p, a)) $$\n",
    "for all $(p, q) \\in P \\times Q$ and $a \\in A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb1e4bea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "implementation (generic function with 3 methods)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function implementation(\n",
    "    learner::CompositeLearner{M, H, N},\n",
    "    input::Vector{Float64}\n",
    ") where {M, H, N}\n",
    "    @assert size(input) == (M,)\n",
    "    return implementation(learner.left, implementation(learner.right, input))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cccf10",
   "metadata": {},
   "source": [
    "A problem arises when trying to create a composite update function. Recall that the update function $U$ requires an observation $(a, b) \\in A \\times B$ and the update function $V$ requires an observation $(b, c) \\in B \\times C$. But, when training our composite learner—whose input space is $A$ and output space is $C$—we only receive observations $(a, c) \\in A \\times C$. A natural choice for the observation from $B \\times C$ is $(I(p, a), c)$ because $I(p, a) \\in B$ is the intermediate value after applying only the first learner $(P, I, U, r)$.\n",
    "\n",
    "The request function $s : Q \\times B \\times C \\to B$ is introduced to provide the observation $(a, s(q, b, c))$ from $A \\times B$. This function should be designed in a such a way that it usually returns an element $b' = s(q, b, c) \\in B$ with $J(q, b')$ closer to $c$ than the original $J(q, b)$. This is achieved by setting\n",
    "$$ s(q, b, c) = f_b (\\nabla_a E_J(q, b, c)) $$\n",
    "where $f_b$ denotes the inverse of the componentwise function $\\frac{\\partial e}{\\partial x}(b_i, \\cdot)$. Note that the request function for $(P, I, U, r)$ is defined similarly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcbfc0ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "request (generic function with 2 methods)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function request(\n",
    "    learner::LinearLearner{M, N},\n",
    "    input::Vector{Float64},\n",
    "    expected::Vector{Float64}\n",
    ") where {M, N}\n",
    "    @assert size(input) == (M,)\n",
    "    @assert size(expected) == (N,)\n",
    "\n",
    "    predicted = implementation(learner, input)\n",
    "    return input - transpose(learner.weights) * (predicted - expected)\n",
    "end\n",
    "\n",
    "function request(\n",
    "    learner::SigmoidLearner{N},\n",
    "    input::Vector{Float64},\n",
    "    expected::Vector{Float64}\n",
    ") where {N}\n",
    "    @assert size(input) == (N,)\n",
    "    @assert size(expected) == (N,)\n",
    "\n",
    "    predicted = implementation(learner, input)\n",
    "    return input - (predicted - expected) .* Dσ.(input)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af819c64",
   "metadata": {},
   "source": [
    "After defining the request functions for our `LinearLearner{M, N}` and `SigmoidLearner{N}`, we are now able to implement the update function for `CompositeLearner{M, H, N}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "306bf5b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "update (generic function with 3 methods)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function update(\n",
    "    learner::CompositeLearner{M, H, N},\n",
    "    input::Vector{Float64},\n",
    "    expected::Vector{Float64}\n",
    ") where {M, H, N}\n",
    "    @assert size(input) == (M,)\n",
    "    @assert size(expected) == (N,)\n",
    "    \n",
    "    forward = implementation(learner.right, input)\n",
    "    left = update(learner.left, forward, expected)\n",
    "\n",
    "    backward = request(learner.left, forward, expected)\n",
    "    right = update(learner.right, input, backward)\n",
    "\n",
    "    return left ∘ right\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce40d6ed",
   "metadata": {},
   "source": [
    "But, if we were to create and train a complicated learner, we would encounter an error. Namely, we have not yet implemented a request function for the `CompositeLearner{M, H, N}` type. The natural choice for this request function is\n",
    "$$ r(p, a, s(q, I(p, a), c)). $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0109faef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "request (generic function with 3 methods)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function request(\n",
    "    learner::CompositeLearner{M, H, N},\n",
    "    input::Vector{Float64},\n",
    "    expected::Vector{Float64}\n",
    ") where {M, H, N}\n",
    "    @assert size(input) == (M,)\n",
    "    @assert size(expected) == (N,)\n",
    "\n",
    "    forward = implementation(learner.right, input)\n",
    "    return request(learner.right, input, request(learner.left, forward, expected))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ec4b73",
   "metadata": {},
   "source": [
    "Again, we repeat a small sanity check to ensure that these composite learners are able to successfully learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb19f162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Weights: [0.41 0.86; 0.07 0.09]\n",
      "Target Biases: [0.66, 0.12]\n",
      "\n",
      "Trained Weights: [0.42 0.85; 0.09 0.11]\n",
      "Trained Biases: [0.66, 0.1]\n"
     ]
    }
   ],
   "source": [
    "Random.seed!(0)\n",
    "\n",
    "input_size = 2\n",
    "output_size = 2\n",
    "samples = 100\n",
    "epochs = 100\n",
    "\n",
    "# Target Function\n",
    "W = round.(rand(output_size, input_size); digits=2)\n",
    "b = round.(rand(output_size); digits=2)\n",
    "f(x) = σ.(W * x + b)\n",
    "\n",
    "# Training\n",
    "data = [(input=Vector(x), output=f(Vector(x))) for x in eachcol(rand(input_size, samples))]\n",
    "learner = SigmoidLearner{2}() ∘ LinearLearner{2,2}()\n",
    "result = train(learner, data, epochs)\n",
    "\n",
    "println(\"Target Weights: $(W)\")\n",
    "println(\"Target Biases: $(b)\")\n",
    "println()\n",
    "println(\"Trained Weights: $(round.(result.right.weights; digits=2))\")\n",
    "println(\"Trained Biases: $(round.(result.right.biases; digits=2))\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.1",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
